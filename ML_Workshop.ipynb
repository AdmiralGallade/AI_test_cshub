{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Workshop",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdmiralGallade/AI_test_cshub/blob/master/ML_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhaHSXillAnQ",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Read data\n",
        "* Use PANDAS to read the csv files into training and test sets, which are now Dataframe objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce4vMkY9XZ42",
        "colab_type": "code",
        "outputId": "67361a5e-47aa-481d-ed98-74bbbb2f12e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "train = pd.read_csv('/content/sample_data/mnist_train_small.csv', delimiter = \",\")\n",
        "test = pd.read_csv('/content/sample_data/mnist_test.csv', delimiter = \",\")\n",
        "\n",
        "!pip install karas --upgrade\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: karas in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from karas) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard in /usr/local/lib/python3.6/dist-packages (from karas) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from karas) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboardx<=1.5 in /usr/local/lib/python3.6/dist-packages (from karas) (1.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from karas) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->karas) (0.16.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->karas) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->karas) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->karas) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->karas) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->karas) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->karas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYG5Nn2y2kGz",
        "colab_type": "text"
      },
      "source": [
        "Show the first and last 5 rows of data as a preview."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W63asi-G2cJH",
        "colab_type": "code",
        "outputId": "87814731-203c-41e2-bec9-852dec1da99a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "train.head"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of        6  0  0.1  0.2  0.3  0.4  ...  0.585  0.586  0.587  0.588  0.589  0.590\n",
              "0      5  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "1      7  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "2      9  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "3      5  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "4      2  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "...   .. ..  ...  ...  ...  ...  ...    ...    ...    ...    ...    ...    ...\n",
              "19994  0  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "19995  1  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "19996  2  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "19997  9  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "19998  5  0    0    0    0    0  ...      0      0      0      0      0      0\n",
              "\n",
              "[19999 rows x 785 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkPuSK73k_bs",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Separate into X(input) and y(label)\n",
        "* Separate the training set into X_train and y_train using iloc, which slices Dataframe by indices.  \n",
        "* Separate the test set into X_test and y_test as well.\n",
        "* Use .to_categorical to convert the y_train into a matrix of 10 classes, which are the classes of digits we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCiDznDLkyYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "X_train = train.iloc[:, 1:]\n",
        "y_train = train.iloc[:, 0]\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "\n",
        "X_test = test.iloc[:, 1:]\n",
        "y_test = test.iloc[:, 0]\n",
        "\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-dfj7OZmHb4",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Create the model\n",
        "* Create a sequential model using the Sequential constructor of Keras. It is an easy way to create a model by adding layers.\n",
        "* We use dense layers for all our layers. It is a type of layer that has all nodes connected between layers. \n",
        "* Use sigmoid function as the activation function and 64 nodes for each hidden layer. \n",
        "* Use softmax function as the activation function for the output. There are 10 output nodes.\n",
        "\n",
        "An **activation function** is used by classifiers to map the output values of the previous layer to the next layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE00TkIQmHGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "a = 'sigmoid'\n",
        "n = 64\n",
        "\n",
        "model.add(Dense(n, input_dim=784, activation=a)) #input\n",
        "for _ in range(2): #hidden layers\n",
        "  model.add(Dense(n, activation=a))\n",
        "model.add(Dense(10, activation = 'softmax')) #output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4dix3tmmOzq",
        "colab_type": "code",
        "outputId": "fdc04455-bb58-4fce-c9ab-3ec3f0906a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 59,210\n",
            "Trainable params: 59,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqVeUuJhmPfG",
        "colab_type": "text"
      },
      "source": [
        "Training the model is not enough to determine how well our model does for unknown datasets. For that, we need cross validation.  \n",
        "\n",
        "## Step 4: Split the validation set\n",
        "* Split the training set into cross validation and training set using train_test_split from Scikit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ShEEq0mmUxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rauMVqeVmfqF",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Train and cross-validate model\n",
        "* Compile the model with the loss function, optimizer(and learning rate) and select accuracy as metrics.\n",
        "* Set the model to be trained for 10 epochs and with the batch size of 10.\n",
        "\n",
        "A neural network works by passing the input through the nodes with a set of parameters to output some values to the next layer.  \n",
        "At the final layer, the nodes output the values that classify the input into one of the classes(10 in this case).  \n",
        "\n",
        "The **loss function** is used to calculate how much the output of our model differs with the actual answer.\n",
        "The **optimizer** is used to find the parameters that produces the lowest 'loss'.\n",
        "\n",
        "Here we use [RMSprop](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a) and [categorical crossentropy](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html).\n",
        "\n",
        "The **epoch** is how many times the entire dataset passes through the neural network.  \n",
        "The **batch size** is the size of the part of the input that updates the parameters. Larger batch sizes are more accurate but require more memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zic3F6Hn9AX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0XfvXE_mhYp",
        "colab_type": "code",
        "outputId": "64334969-aad2-40c5-c70d-f5843a973471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "from keras.optimizers import RMSprop \n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(0.01), metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=10, validation_data=(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 17999 samples, validate on 2000 samples\n",
            "Epoch 1/20\n",
            "17999/17999 [==============================] - 4s 203us/step - loss: 1.0874 - accuracy: 0.6212 - val_loss: 0.8506 - val_accuracy: 0.7135\n",
            "Epoch 2/20\n",
            "17999/17999 [==============================] - 4s 198us/step - loss: 0.7367 - accuracy: 0.7658 - val_loss: 0.6528 - val_accuracy: 0.8065\n",
            "Epoch 3/20\n",
            "17999/17999 [==============================] - 3s 194us/step - loss: 0.6293 - accuracy: 0.8144 - val_loss: 0.6245 - val_accuracy: 0.8005\n",
            "Epoch 4/20\n",
            "17999/17999 [==============================] - 4s 195us/step - loss: 0.5911 - accuracy: 0.8308 - val_loss: 0.5727 - val_accuracy: 0.8345\n",
            "Epoch 5/20\n",
            "17999/17999 [==============================] - 4s 211us/step - loss: 0.5819 - accuracy: 0.8414 - val_loss: 0.5659 - val_accuracy: 0.8555\n",
            "Epoch 6/20\n",
            "17999/17999 [==============================] - 4s 216us/step - loss: 0.5749 - accuracy: 0.8492 - val_loss: 0.8038 - val_accuracy: 0.8030\n",
            "Epoch 7/20\n",
            "17999/17999 [==============================] - 4s 197us/step - loss: 0.5821 - accuracy: 0.8491 - val_loss: 0.6038 - val_accuracy: 0.8275\n",
            "Epoch 8/20\n",
            "17999/17999 [==============================] - 4s 201us/step - loss: 0.5858 - accuracy: 0.8505 - val_loss: 0.6459 - val_accuracy: 0.8250\n",
            "Epoch 9/20\n",
            "17999/17999 [==============================] - 4s 198us/step - loss: 0.5775 - accuracy: 0.8542 - val_loss: 0.6941 - val_accuracy: 0.8375\n",
            "Epoch 10/20\n",
            "17999/17999 [==============================] - 4s 198us/step - loss: 0.5849 - accuracy: 0.8562 - val_loss: 0.5944 - val_accuracy: 0.8410\n",
            "Epoch 11/20\n",
            "17999/17999 [==============================] - 4s 201us/step - loss: 0.5960 - accuracy: 0.8524 - val_loss: 0.5976 - val_accuracy: 0.8665\n",
            "Epoch 12/20\n",
            "17999/17999 [==============================] - 4s 209us/step - loss: 0.5720 - accuracy: 0.8614 - val_loss: 0.6101 - val_accuracy: 0.8510\n",
            "Epoch 13/20\n",
            "17999/17999 [==============================] - 4s 207us/step - loss: 0.5850 - accuracy: 0.8638 - val_loss: 0.6208 - val_accuracy: 0.8630\n",
            "Epoch 14/20\n",
            "17999/17999 [==============================] - 4s 202us/step - loss: 0.5791 - accuracy: 0.8717 - val_loss: 0.5389 - val_accuracy: 0.8750\n",
            "Epoch 15/20\n",
            "17999/17999 [==============================] - 4s 199us/step - loss: 0.5843 - accuracy: 0.8703 - val_loss: 0.6951 - val_accuracy: 0.8595\n",
            "Epoch 16/20\n",
            "17999/17999 [==============================] - 4s 200us/step - loss: 0.6067 - accuracy: 0.8697 - val_loss: 0.7415 - val_accuracy: 0.8570\n",
            "Epoch 17/20\n",
            "17999/17999 [==============================] - 4s 203us/step - loss: 0.5880 - accuracy: 0.8788 - val_loss: 0.5349 - val_accuracy: 0.8705\n",
            "Epoch 18/20\n",
            "17999/17999 [==============================] - 3s 194us/step - loss: 0.6115 - accuracy: 0.8755 - val_loss: 0.6439 - val_accuracy: 0.8720\n",
            "Epoch 19/20\n",
            "17999/17999 [==============================] - 4s 204us/step - loss: 0.6201 - accuracy: 0.8766 - val_loss: 0.7088 - val_accuracy: 0.8780\n",
            "Epoch 20/20\n",
            "17999/17999 [==============================] - 4s 200us/step - loss: 0.6102 - accuracy: 0.8741 - val_loss: 0.7016 - val_accuracy: 0.8745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fe76ee5cf98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n9iCkHwmoMC",
        "colab_type": "text"
      },
      "source": [
        "### There we have it, a very basic neural network. \n",
        "It is about 80% accurate. Which is actually not a lot as an accurate model needs to be more than 95% accurate and additional steps are needed to improve the accuracy. (We will go over if we have time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRmXFgfMnZ_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5hi-gtZnZek",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBnPS9ksmqRl",
        "colab_type": "text"
      },
      "source": [
        "## Predict the test set\n",
        "* model.predict will return a 10 column matrix. Each column describes the probabilty of the input being a certain digit.\n",
        "* We use np.argmax on the row axis to get the highest probabilty, which predicts the digit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaGVsFI6y7rD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_prob = model.predict(X_test)\n",
        "y_prediction = np.argmax(y_prob, axis=1) # takes in the largest probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHSFAO6q_hpe",
        "colab_type": "text"
      },
      "source": [
        "So we have y_test from before, which is the actual 'answer' for the test set.  \n",
        "\n",
        "We can use it to compare with our prediction to see how accurate our prediction is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKUpOocS3Msn",
        "colab_type": "code",
        "outputId": "f34e9ee8-6a67-4789-eb45-92a6d9d44bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(y_prediction)\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 1 0 ... 4 5 6]\n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeC23O-HEtRs",
        "colab_type": "text"
      },
      "source": [
        "##Export the model\n",
        "* We save the configuration of the model into JSON format so that we can download and use it.\n",
        "* We then save the weights of the model into h5 format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuAOvcJEET04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open('/content/model.json', 'w') as f:\n",
        "  f.write(model_json)\n",
        "model.save_weights('/content/model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE4QhITEGXII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp-htSVBGsJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}